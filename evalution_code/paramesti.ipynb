{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import anthropic\n",
    "import together\n",
    "import subprocess\n",
    "import re\n",
    "\n",
    "# open ai API key\n",
    "openai_api_key=''\n",
    "# anthropic API key\n",
    "claude_api_key = ''\n",
    "# together AI API key\n",
    "together_api_key = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You're an expert in stochastic modeling and you're tasked to solve the following problem.\n",
      "\n",
      "Objective:\n",
      "----------\n",
      "Estimate the unknown parameter vector of a two-dimensional gamma distribution using maximum likelihood estimation.\n",
      "\n",
      "Problem Description:\n",
      "--------------------\n",
      "A simulation generates i.i.d. observations Y_j = (y_{1,j}, y_{2,j}) from a two-dimensional gamma distribution defined over [0, infinity) x [0, infinity). The density function is given by\n",
      "\n",
      "    f(y_1, y_2; x*) = (exp(-y_1) * y_1^(x*_1 * y_2 - 1)) / Gamma(x*_1 * y_2) *\n",
      "                       (exp(-y_2) * y_2^(x*_2 - 1)) / Gamma(x*_2),\n",
      "    for y_1, y_2 > 0,\n",
      "\n",
      "where the true parameter vector x* = (x*_1, x*_2) is unknown. The function\n",
      "\n",
      "    g(x) = E[ log( f(Y; x) ) ] = integral_0^infinity log( f(y; x) ) * f(y; x*) dy\n",
      "\n",
      "is maximized by x*, and its sample estimator\n",
      "\n",
      "    G_m(x) = (1/m) * sum_(j=1)^m log( f(Y_j; x) )\n",
      "\n",
      "is consistent. In this context, each component of Y_j is a gamma random variable, and the goal is to recover x* through optimization.\n",
      "\n",
      "Given Model Factors (defaults in parentheses):\n",
      "----------------------------------------------\n",
      "- True Parameters (x*): The unknown parameter vector that maximizes the expected log likelihood.\n",
      "  Default: [2, 5]\n",
      "\n",
      "- Parameter Guess (x): The decision variable used in the log likelihood evaluation.\n",
      "  Starting value: [1, 1]\n",
      "\n",
      "Randomness Sources:\n",
      "-------------------\n",
      "- The observation vector Y = (y_1, y_2) is random, with both y_1 and y_2 being gamma-distributed.\n",
      "\n",
      "Requirements:\n",
      "-------------\n",
      "1. Analytical / Closed-Form Approach\n",
      "   - If a closed-form solution or estimator for x* exists, derive and explain it.\n",
      "   - Justify any assumptions or approximations used in the derivation.\n",
      "\n",
      "2. Simulation-Based Approach\n",
      "   - If a closed-form solution is not feasible or if additional verification is needed:\n",
      "     - Develop a simulation model that, for a given x, estimates the sample log likelihood G_m(x).\n",
      "     - Propose an optimization strategy (e.g., direct search, gradient-based methods, or other heuristic methods) to find the x that maximizes G_m(x) (or equivalently minimizes the negative log likelihood).\n",
      "     - You are allowed a limited budget of [BUDGET] objective function evaluations (default: 1000).\n",
      "   - Coding Instructions:\n",
      "     If you provide code, enclose it in one single fenced code block using:\n",
      "       \n",
      "       ```python\n",
      "       # ...your code...\n",
      "       ```\n",
      "       \n",
      "     Ensure the code:\n",
      "       - Generates random observations Y_j from the specified gamma distributions.\n",
      "       - Computes the log likelihood for a given x.\n",
      "       - Explores or optimizes x within the allowed [BUDGET].\n",
      "\n",
      "3. Performance Measures & Validation\n",
      "   - Report the estimated log likelihood and the corresponding parameter estimate x.\n",
      "   - If using simulation, provide variance estimates or confidence intervals for the log likelihood.\n",
      "   - Compare the performance of your proposed solution to a baseline (e.g., using the initial guess x = [1, 1]) to demonstrate improvement.\n",
      "\n",
      "Deliverables:\n",
      "-------------\n",
      "1. Solution Explanation\n",
      "   - Clearly explain the method used (closed-form derivation or simulation-based optimization), the resulting estimated parameter vector, and your justification.\n",
      "2. If Submitting Code\n",
      "   - Provide a single, self-contained, runnable Python code block that shows how the data is generated, the log likelihood is calculated, and x is optimized.\n",
      "3. Optimization Results\n",
      "   - Present the recommended estimate for x.\n",
      "   - Include numerical results such as the estimated log likelihood, confidence intervals, or other relevant metrics.\n",
      "\n",
      "Testing Notes:\n",
      "-------------\n",
      "- You have a budget of [BUDGET] (e.g., 1000 evaluations) for objective function computations.\n",
      "- Random candidate solutions for x can be generated uniformly in the square (0, 10) x (0, 10).\n",
      "- The evaluation will focus on how well your method recovers the parameter vector and improves over the baseline.\n",
      "\n",
      "Success Criteria:\n",
      "-----------------\n",
      "1. Feasibility: The estimated x must lie within the square (0, 10) x (0, 10).\n",
      "2. Demonstrated Improvement: The log likelihood obtained should be higher than that of trivial solutions (such as the starting guess x = [1, 1]).\n",
      "3. Justification: Provide sound reasoning and numerical evidence for your recommended estimate.\n",
      "\n",
      "Final Instructions:\n",
      "------------------\n",
      "- If using a simulation approach, explain your method and justify your choices.\n",
      "- If a closed-form or near-closed-form solution is derived, provide the derivation and validate it accordingly.\n"
     ]
    }
   ],
   "source": [
    "# read the prompt from the prompt text file titled \"paramesti.txt\"\n",
    "with open(\"../prompts/paramesti.txt\", \"r\") as f:\n",
    "    prompt = f.read()\n",
    "\n",
    "prompt = str(prompt)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "claude_client = anthropic.Anthropic(api_key = claude_api_key)\n",
    "openai_client = openai.OpenAI(api_key = openai_api_key)\n",
    "togetherai_client = together.Together(api_key=together_api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT-4o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = openai_client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are an expert in probability theory and stochastic modeling.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    ")\n",
    "\n",
    "# get the response\n",
    "response = completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To tackle this problem, we can employ a simulation-based approach since the joint density function given does not readily lend itself to a closed-form maximum likelihood solution due to the complexity introduced by the interaction between the two components of the vector Y in the gamma distributions.\n",
      "\n",
      "The workflow is divided into these key steps:\n",
      "\n",
      "1. **Data Simulation**: Generate i.i.d. samples from the defined two-dimensional gamma distribution using the known parameter vector \\( x^* \\).\n",
      "\n",
      "2. **Log Likelihood Computation**: For given parameter candidates \\( x = (x_1, x_2) \\), compute the sample log likelihood function \\( G_m(x) \\).\n",
      "\n",
      "3. **Optimization**: Apply an optimization method that efficiently searches for the \\( x \\) that maximizes \\( G_m(x) \\).\n",
      "\n",
      "Below is a Python implementation encapsulating the above steps:\n",
      "\n",
      "```python\n",
      "import numpy as np\n",
      "from scipy.special import gamma, gammaln\n",
      "from scipy.optimize import minimize\n",
      "\n",
      "# Step 1: Data generation function\n",
      "def generate_data(n_samples, x_star):\n",
      "    y2 = np.random.gamma(shape=x_star[1], scale=1, size=n_samples)\n",
      "    y1 = np.random.gamma(shape=x_star[0] * y2, scale=1)\n",
      "    return np.vstack((y1, y2)).T\n",
      "\n",
      "# Step 2: Log likelihood computation\n",
      "def sample_log_likelihood(X, data):\n",
      "    y1, y2 = data[:, 0], data[:, 1]\n",
      "    x1, x2 = X\n",
      "\n",
      "    term1 = -y1 + (x1 * y2 - 1) * np.log(y1) - gammaln(x1 * y2)\n",
      "    term2 = -y2 + (x2 - 1) * np.log(y2) - gammaln(x2)\n",
      "    \n",
      "    log_likelihood = np.sum(term1 + term2)\n",
      "    return log_likelihood\n",
      "\n",
      "# Step 3: Optimization function\n",
      "def optimize_mle(data, initial_guess, budget):\n",
      "    def objective(X):\n",
      "        # Negative because we are maximizing\n",
      "        return -sample_log_likelihood(X, data)\n",
      "    \n",
      "    bounds = [(0.1, 10), (0.1, 10)]\n",
      "    result = minimize(objective, initial_guess, bounds=bounds, options={'maxiter': budget, 'disp': True})\n",
      "    return result.x, -result.fun\n",
      "\n",
      "# Parameters\n",
      "true_parameters = np.array([2, 5])\n",
      "initial_guess = np.array([1, 1])\n",
      "n_samples = 1000\n",
      "budget = 1000\n",
      "\n",
      "# Run data generation\n",
      "data = generate_data(n_samples, true_parameters)\n",
      "\n",
      "# Perform optimization\n",
      "estimated_parameters, estimated_log_likelihood = optimize_mle(data, initial_guess, budget)\n",
      "\n",
      "# Output results\n",
      "print(f\"Estimated parameters: {estimated_parameters}\")\n",
      "print(f\"Estimated log likelihood: {estimated_log_likelihood}\")\n",
      "```\n",
      "\n",
      "### Explanation of the Method:\n",
      "\n",
      "1. **Data Generation**: The Y vector is generated by first simulating \\( y_2 \\) from a gamma distribution with parameters \\( (x^*_2, 1) \\), then \\( y_1 \\) from gamma with scale parameter 1 and shape dependent on \\( x^*_1 \\cdot y_2 \\).\n",
      "\n",
      "2. **Log Likelihood**: The sample log likelihood is computed using the gamma functions to reflect the probability density functions of each component as expressed in the problem statement.\n",
      "\n",
      "3. **Optimization**: We use the `scipy.optimize.minimize` function with the 'L-BFGS-B' method because it's suitable for bounds-constrained optimization typically efficient for this type of problem. The objective function is the negative log likelihood (since `minimize` by default minimizes), bounds are set to ensure the parameters remain in the feasible range.\n",
      "\n",
      "4. **Results**: Estimates for the parameters and log likelihood are obtained and displayed, with expectations that the solution improves on using the initial guess and remains within the bounds specified.\n",
      "\n",
      "### Performance Measures:\n",
      "To assert improvement, we compare calculated log likelihoods especially starting from different initial parameters and showing the results are consistently better than trivial solutions within the constraints and objective evaluations.\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated parameters: [2.00076309 5.02050859]\n",
      "Estimated log likelihood: -4611.680006069343\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.special import gamma, gammaln\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# Step 1: Data generation function\n",
    "def generate_data(n_samples, x_star):\n",
    "    y2 = np.random.gamma(shape=x_star[1], scale=1, size=n_samples)\n",
    "    y1 = np.random.gamma(shape=x_star[0] * y2, scale=1)\n",
    "    return np.vstack((y1, y2)).T\n",
    "\n",
    "# Step 2: Log likelihood computation\n",
    "def sample_log_likelihood(X, data):\n",
    "    y1, y2 = data[:, 0], data[:, 1]\n",
    "    x1, x2 = X\n",
    "\n",
    "    term1 = -y1 + (x1 * y2 - 1) * np.log(y1) - gammaln(x1 * y2)\n",
    "    term2 = -y2 + (x2 - 1) * np.log(y2) - gammaln(x2)\n",
    "    \n",
    "    log_likelihood = np.sum(term1 + term2)\n",
    "    return log_likelihood\n",
    "\n",
    "# Step 3: Optimization function\n",
    "def optimize_mle(data, initial_guess, budget):\n",
    "    def objective(X):\n",
    "        # Negative because we are maximizing\n",
    "        return -sample_log_likelihood(X, data)\n",
    "    \n",
    "    bounds = [(0.1, 10), (0.1, 10)]\n",
    "    result = minimize(objective, initial_guess, bounds=bounds, options={'maxiter': budget, 'disp': True})\n",
    "    return result.x, -result.fun\n",
    "\n",
    "# Parameters\n",
    "true_parameters = np.array([2, 5])\n",
    "initial_guess = np.array([1, 1])\n",
    "n_samples = 1000\n",
    "budget = 1000\n",
    "\n",
    "# Run data generation\n",
    "data = generate_data(n_samples, true_parameters)\n",
    "\n",
    "# Perform optimization\n",
    "estimated_parameters, estimated_log_likelihood = optimize_mle(data, initial_guess, budget)\n",
    "\n",
    "# Output results\n",
    "print(f\"Estimated parameters: {estimated_parameters}\")\n",
    "print(f\"Estimated log likelihood: {estimated_log_likelihood}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## o1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = openai_client.chat.completions.create(\n",
    "    model=\"o1\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are an expert in probability theory and stochastic modeling.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    ")\n",
    "\n",
    "# get the response\n",
    "response = completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solution Explanation\n",
      "====================\n",
      "\n",
      "We have i.i.d. samples Y₍ⱼ₎ = (y₁,ⱼ, y₂,ⱼ) from the density\n",
      "\n",
      "  f(y₁, y₂; x) \n",
      "  = [ exp(-y₁) · y₁^(x₁·y₂ - 1 ) / Γ(x₁·y₂ ) ] \n",
      "    × [ exp(-y₂) · y₂^(x₂ - 1 )        / Γ(x₂)       ],\n",
      "  \n",
      "where x = (x₁, x₂) is the unknown parameter vector to be estimated. Below is a two-part solution:\n",
      "\n",
      "1. Partially Closed-Form MLE Approach\n",
      "2. Simulation and Numerical Validation\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "1. Partially Closed-Form MLE Approach\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "▌ Log-Likelihood for One Sample\n",
      "\n",
      "The log of f(y₁, y₂; x) decomposes into two parts:\n",
      "\n",
      "  log f(y₁, y₂; x) \n",
      "   =  log [ exp(-y₁) · y₁^(x₁y₂ - 1 ) / Γ(x₁y₂) ]\n",
      "     + log [ exp(-y₂) · y₂^(x₂ - 1  ) / Γ(x₂)     ].\n",
      "\n",
      "Expanding and regrouping:\n",
      "\n",
      "  = -y₁ + (x₁y₂ - 1) log(y₁) - log Γ(x₁y₂)\n",
      "    -y₂ + (x₂ - 1)   log(y₂) - log Γ(x₂).\n",
      "\n",
      "Given m independent observations {(y₁,ⱼ, y₂,ⱼ)}ⱼ₌₁..m, the log-likelihood is:\n",
      "\n",
      "  L(x) = ∑(ⱼ=1..m) log f(y₁,ⱼ, y₂,ⱼ; x).\n",
      "\n",
      "Then the sample average log-likelihood (i.e., Gₘ(x)) is just L(x)/m, but maximizing Gₘ(x) is equivalent to maximizing L(x).\n",
      "\n",
      "▌ Setting Up the MLE Equations\n",
      "\n",
      "We take partial derivatives of L(x) with respect to x₁ and x₂, and set them to zero.\n",
      "\n",
      "● Derivative w.r.t. x₂\n",
      "\n",
      "Consider the x₂-dependent terms:\n",
      "\n",
      "  Σⱼ ( (x₂ - 1) log(y₂,ⱼ) - log Γ(x₂) ).\n",
      "\n",
      "Its derivative w.r.t. x₂ is:\n",
      "\n",
      "  Σⱼ [ log(y₂,ⱼ) ] - m ψ(x₂),\n",
      "\n",
      "where ψ(·) is the digamma function, the derivative of log Γ(·). Setting that to zero:\n",
      "\n",
      "  0 \n",
      "  = ∑ⱼ=1..m [ log(y₂,ⱼ) ] - m ψ(x₂)\n",
      "  ⇒ ψ(x₂) = (1/m) ∑ⱼ=1..m log(y₂,ⱼ).\n",
      "\n",
      "This is precisely the usual shape-parameter MLE equation for a univariate Gamma(·, 1). Although there is no elementary closed-form for ψ⁻¹, we can numerically invert ψ to find x₂. Many scientific libraries have special functions that directly compute this inversion or it can be solved with a simple 1D root-finding.\n",
      "\n",
      "● Derivative w.r.t. x₁\n",
      "\n",
      "For the x₁-dependent terms:\n",
      "\n",
      "  Σⱼ [ (x₁ y₂,ⱼ - 1) log(y₁,ⱼ) - log Γ(x₁ y₂,ⱼ ) ].\n",
      "\n",
      "Taking the derivative w.r.t. x₁:\n",
      "\n",
      "  Σⱼ [ y₂,ⱼ · log(y₁,ⱼ) ] - Σⱼ [ y₂,ⱼ · ψ(x₁ y₂,ⱼ) ].\n",
      "\n",
      "Set this equal to zero:\n",
      "\n",
      "  Σⱼ [ y₂,ⱼ · log(y₁,ⱼ) ] \n",
      "  = Σⱼ [ y₂,ⱼ · ψ(x₁ y₂,ⱼ) ].\n",
      "\n",
      "Define\n",
      "\n",
      "  H(x₁) = ∑ⱼ [ y₂,ⱼ ψ(x₁ y₂,ⱼ) ]  -  ∑ⱼ [ y₂,ⱼ log(y₁,ⱼ) ].\n",
      "\n",
      "We want H(x₁) = 0. Because y₂,ⱼ ≥ 0 and the digamma function ψ(z) is monotonic in z>0, one can typically solve H(x₁) = 0 with a simple 1D root-finding (bisection, Newton, etc.). The derivative of H(x₁) with respect to x₁,\n",
      "\n",
      "  H′(x₁) = ∑ⱼ [ y₂,ⱼ²  · ψ′(x₁ y₂,ⱼ) ],\n",
      "\n",
      "is strictly positive (since ψ′(·) = trigamma(·) > 0 for arguments > 0), so H(x₁) is an increasing function, guaranteeing a unique root.\n",
      "\n",
      "Hence the MLE can be found via:\n",
      "\n",
      "  1) Find x₂ by solving  ψ(x₂) = (1/m) ∑ⱼ log(y₂,ⱼ).\n",
      "  2) Find x₁ by solving  ∑ⱼ [ y₂,ⱼ ψ(x₁ y₂,ⱼ) ] = ∑ⱼ [ y₂,ⱼ log(y₁,ⱼ) ].\n",
      "\n",
      "This approach—although not purely \"closed-form\"—uses standard equations for gamma MLEs (for x₂) plus one-dimensional root search (for x₁).\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "2. Simulation and Numerical Validation\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Below is a single runnable Python script that:\n",
      "\n",
      "1. Generates data Yⱼ from the given 2D model with true parameters x* = (2, 5).  \n",
      "2. Implements the log-likelihood.  \n",
      "3. Solves for (x₁, x₂) via:\n",
      "   – A direct application of the above partial-closed-form approach, or  \n",
      "   – Alternatively, a direct numerical maximization (“black-box”) if desired.  \n",
      "4. Prints the estimated parameters and compares them to the truth.\n",
      "\n",
      "You can adjust “BUDGET” (default 1000) if you prefer a more general optimization method.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Code\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "```python\n",
      "import numpy as np\n",
      "import scipy.special as sps\n",
      "from scipy.optimize import bisect, root_scalar\n",
      "\n",
      "def gamma2d_sample(x_true, m, rng=None):\n",
      "    \"\"\"\n",
      "    Generate m i.i.d. samples from the 2D Gamma distribution:\n",
      "      y1 ~ Gamma(shape = x1 * y2, rate=1)\n",
      "      y2 ~ Gamma(shape = x2,        rate=1).\n",
      "    Since y2 is independent of y1, we first sample y2, then y1 conditional on y2.\n",
      "    \"\"\"\n",
      "    if rng is None:\n",
      "        rng = np.random.default_rng()\n",
      "    x1_true, x2_true = x_true\n",
      "    # Sample y2 ~ Gamma(shape=x2_true, scale=1)\n",
      "    y2 = rng.gamma(shape=x2_true, scale=1.0, size=m)\n",
      "    # For each y2_j, sample y1_j ~ Gamma(shape=x1_true*y2_j, scale=1)\n",
      "    y1 = np.array([rng.gamma(shape=x1_true * yy, scale=1.0) for yy in y2])\n",
      "    return np.column_stack((y1, y2))\n",
      "\n",
      "def log_likelihood(x, data):\n",
      "    \"\"\"\n",
      "    Computes the log-likelihood sum_j log f(y_j; x).\n",
      "    data is an (m,2) array with columns (y1_j, y2_j).\n",
      "    \"\"\"\n",
      "    x1, x2 = x\n",
      "    y1 = data[:,0]\n",
      "    y2 = data[:,1]\n",
      "    # log f(y1, y2; x) = ...\n",
      "    # part1 = -y1 + (x1*y2 -1)*log(y1) - log Gamma(x1*y2)\n",
      "    # part2 = -y2 + (x2   -1)*log(y2) - log Gamma(x2)\n",
      "    ll_part1 = -y1 + (x1*y2 - 1)*np.log(y1) - sps.gammaln(x1*y2)\n",
      "    ll_part2 = -y2 + (x2 - 1)*np.log(y2)    - sps.gammaln(x2)\n",
      "    return np.sum(ll_part1 + ll_part2)\n",
      "\n",
      "def digamma_inverse(target, bracket=(1e-3, 100.0)):\n",
      "    \"\"\"\n",
      "    Solve psi(z) = target for z>0 using a bisection within bracket.\n",
      "    Adjust bracket if needed for your data.\n",
      "    \"\"\"\n",
      "    def f(z):\n",
      "        return sps.digamma(z) - target\n",
      "    return bisect(f, bracket[0], bracket[1])\n",
      "\n",
      "def solve_for_x2(data):\n",
      "    \"\"\"\n",
      "    Solve the MLE equation for x2:\n",
      "      digamma(x2) = average of log(y2).\n",
      "    \"\"\"\n",
      "    y2 = data[:,1]\n",
      "    avg_log_y2 = np.mean(np.log(y2))\n",
      "    # x2 solves psi(x2) = avg_log_y2\n",
      "    # We'll guess a bracket e.g. (1e-3, 100); or refine if data is large\n",
      "    return digamma_inverse(avg_log_y2, bracket=(1e-3, 1e4))\n",
      "\n",
      "def solve_for_x1(data, x1_init=2.0):\n",
      "    \"\"\"\n",
      "    Solve the MLE condition for x1:\n",
      "      sum_j [ y2_j * psi(x1 * y2_j) ] = sum_j [ y2_j * log(y1_j) ].\n",
      "    We'll do a root_scalar with bisection or Newton.\n",
      "    \"\"\"\n",
      "    y1 = data[:,0]\n",
      "    y2 = data[:,1]\n",
      "    sum_y2_logy1 = np.sum(y2 * np.log(y1))\n",
      "\n",
      "    def H(x1):\n",
      "        return np.sum(y2 * sps.digamma(x1 * y2)) - sum_y2_logy1\n",
      "\n",
      "    # We must have x1>0. We'll do a bracket search. \n",
      "    # If data is fairly large, you might want to refine bracket automatically.\n",
      "    bracket_min = 1e-3\n",
      "    bracket_max = 100.0\n",
      "    # Expand bracket if needed:\n",
      "    while H(bracket_max) < 0:\n",
      "        bracket_max *= 2.0\n",
      "        if bracket_max > 1e12:  # safeguard\n",
      "            break\n",
      "    root_result = root_scalar(H, bracket=[bracket_min, bracket_max], method='bisect')\n",
      "    return root_result.root\n",
      "\n",
      "def estimate_parameters_closed_form(data):\n",
      "    \"\"\"\n",
      "    Estimate (x1, x2) via the partial closed-form approach:\n",
      "      1) Solve for x2 from digamma(x2) = average(log(y2)).\n",
      "      2) Solve for x1 from sum_j y2_j psi(x1 y2_j) = sum_j y2_j log(y1_j).\n",
      "    \"\"\"\n",
      "    x2_hat = solve_for_x2(data)\n",
      "    x1_hat = solve_for_x1(data)\n",
      "    return (x1_hat, x2_hat)\n",
      "\n",
      "\n",
      "def main():\n",
      "    # ------------------------------\n",
      "    # 1. Generate data\n",
      "    # ------------------------------\n",
      "    m = 2000                      # number of samples\n",
      "    true_x = (2.0, 5.0)           # true parameter vector\n",
      "    rng = np.random.default_rng(123)\n",
      "    data = gamma2d_sample(true_x, m, rng=rng)\n",
      "\n",
      "    # ------------------------------\n",
      "    # 2. Baseline log-likelihood at x=[1,1]\n",
      "    # ------------------------------\n",
      "    baseline_x = (1.0, 1.0)\n",
      "    ll_baseline = log_likelihood(baseline_x, data)\n",
      "    print(\"Baseline guess x = (1,1)\")\n",
      "    print(\"Log-likelihood(1,1) =\", ll_baseline)\n",
      "\n",
      "    # ------------------------------\n",
      "    # 3. Estimate x via partial closed-form approach\n",
      "    # ------------------------------\n",
      "    x_hat = estimate_parameters_closed_form(data)\n",
      "    ll_hat = log_likelihood(x_hat, data)\n",
      "    print(\"\\nPartial Closed-Form MLE Approach:\")\n",
      "    print(\"Estimated x =\", x_hat)\n",
      "    print(\"Log-likelihood(x_hat) =\", ll_hat)\n",
      "\n",
      "    # ------------------------------\n",
      "    # 4. Compare to the truth\n",
      "    # ------------------------------\n",
      "    print(\"\\nTrue x         =\", true_x)\n",
      "    print(\"Estimated x    =\", x_hat)\n",
      "    print(\"LL difference  =\", ll_hat - ll_baseline)\n",
      "    print(\"Improvement?   =\", \"Yes\" if ll_hat > ll_baseline else \"No\")\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    main()\n",
      "```\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Discussion of Results\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "1. Closed-Form Insight:  \n",
      "   • x₂ can be found by solving ψ(x₂) = average[log(y₂)]. This is the standard shape-parameter MLE for y₂~Gamma(x₂, 1).  \n",
      "   • x₁ must satisfy ∑ⱼ y₂,ⱼ ψ(x₁ y₂,ⱼ) = ∑ⱼ y₂,ⱼ log(y₁,ⱼ). This is a monotonic equation in x₁, so it has a unique positive solution found by 1D root-finding.  \n",
      "\n",
      "2. Simulation and Budget:  \n",
      "   • We used a straightforward partial closed-form approach that only requires a single 1D search for x₁ plus a digamma-inverse for x₂. This is very efficient and typically well within a 1000-evaluation budget.  \n",
      "   • Alternatively, one could maximize log_likelihood(x) using a black-box optimizer (e.g., gradient-based or evolutionary). However, the partial closed-form system is much simpler and more direct here.\n",
      "\n",
      "3. Performance and Validation:  \n",
      "   • The estimated (x₁, x₂) should be close to the true (2, 5) if m is large.  \n",
      "   • The final log-likelihood will exceed the baseline guess of (1,1), thus demonstrating improvement.  \n",
      "   • Confidence intervals (CIs) around (x₁, x₂) could be found by approximate asymptotic variance from the observed Fisher information, or by bootstrap over the data.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Recommended Estimate\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "• The recommended estimate for x = (x₁, x₂) comes from the partial closed-form approach:\n",
      "  (x̂₁, x̂₂).\n",
      "• Numerical tests with large m typically recover x close to (2, 5).  \n",
      "\n",
      "This method provides both a theoretical basis (by analyzing partial derivatives of the log-likelihood) and strong practical performance. If needed, additional verification or finite-sample tuning can be done using the supplied simulation code.\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline guess x = (1,1)\n",
      "Log-likelihood(1,1) = -18196.433743595044\n",
      "\n",
      "Partial Closed-Form MLE Approach:\n",
      "Estimated x = (2.0128422787248335, 4.976167154490353)\n",
      "Log-likelihood(x_hat) = -9226.038105297983\n",
      "\n",
      "True x         = (2.0, 5.0)\n",
      "Estimated x    = (2.0128422787248335, 4.976167154490353)\n",
      "LL difference  = 8970.395638297061\n",
      "Improvement?   = Yes\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.special as sps\n",
    "from scipy.optimize import bisect, root_scalar\n",
    "\n",
    "def gamma2d_sample(x_true, m, rng=None):\n",
    "    \"\"\"\n",
    "    Generate m i.i.d. samples from the 2D Gamma distribution:\n",
    "      y1 ~ Gamma(shape = x1 * y2, rate=1)\n",
    "      y2 ~ Gamma(shape = x2,        rate=1).\n",
    "    Since y2 is independent of y1, we first sample y2, then y1 conditional on y2.\n",
    "    \"\"\"\n",
    "    if rng is None:\n",
    "        rng = np.random.default_rng()\n",
    "    x1_true, x2_true = x_true\n",
    "    # Sample y2 ~ Gamma(shape=x2_true, scale=1)\n",
    "    y2 = rng.gamma(shape=x2_true, scale=1.0, size=m)\n",
    "    # For each y2_j, sample y1_j ~ Gamma(shape=x1_true*y2_j, scale=1)\n",
    "    y1 = np.array([rng.gamma(shape=x1_true * yy, scale=1.0) for yy in y2])\n",
    "    return np.column_stack((y1, y2))\n",
    "\n",
    "def log_likelihood(x, data):\n",
    "    \"\"\"\n",
    "    Computes the log-likelihood sum_j log f(y_j; x).\n",
    "    data is an (m,2) array with columns (y1_j, y2_j).\n",
    "    \"\"\"\n",
    "    x1, x2 = x\n",
    "    y1 = data[:,0]\n",
    "    y2 = data[:,1]\n",
    "    # log f(y1, y2; x) = ...\n",
    "    # part1 = -y1 + (x1*y2 -1)*log(y1) - log Gamma(x1*y2)\n",
    "    # part2 = -y2 + (x2   -1)*log(y2) - log Gamma(x2)\n",
    "    ll_part1 = -y1 + (x1*y2 - 1)*np.log(y1) - sps.gammaln(x1*y2)\n",
    "    ll_part2 = -y2 + (x2 - 1)*np.log(y2)    - sps.gammaln(x2)\n",
    "    return np.sum(ll_part1 + ll_part2)\n",
    "\n",
    "def digamma_inverse(target, bracket=(1e-3, 100.0)):\n",
    "    \"\"\"\n",
    "    Solve psi(z) = target for z>0 using a bisection within bracket.\n",
    "    Adjust bracket if needed for your data.\n",
    "    \"\"\"\n",
    "    def f(z):\n",
    "        return sps.digamma(z) - target\n",
    "    return bisect(f, bracket[0], bracket[1])\n",
    "\n",
    "def solve_for_x2(data):\n",
    "    \"\"\"\n",
    "    Solve the MLE equation for x2:\n",
    "      digamma(x2) = average of log(y2).\n",
    "    \"\"\"\n",
    "    y2 = data[:,1]\n",
    "    avg_log_y2 = np.mean(np.log(y2))\n",
    "    # x2 solves psi(x2) = avg_log_y2\n",
    "    # We'll guess a bracket e.g. (1e-3, 100); or refine if data is large\n",
    "    return digamma_inverse(avg_log_y2, bracket=(1e-3, 1e4))\n",
    "\n",
    "def solve_for_x1(data, x1_init=2.0):\n",
    "    \"\"\"\n",
    "    Solve the MLE condition for x1:\n",
    "      sum_j [ y2_j * psi(x1 * y2_j) ] = sum_j [ y2_j * log(y1_j) ].\n",
    "    We'll do a root_scalar with bisection or Newton.\n",
    "    \"\"\"\n",
    "    y1 = data[:,0]\n",
    "    y2 = data[:,1]\n",
    "    sum_y2_logy1 = np.sum(y2 * np.log(y1))\n",
    "\n",
    "    def H(x1):\n",
    "        return np.sum(y2 * sps.digamma(x1 * y2)) - sum_y2_logy1\n",
    "\n",
    "    # We must have x1>0. We'll do a bracket search. \n",
    "    # If data is fairly large, you might want to refine bracket automatically.\n",
    "    bracket_min = 1e-3\n",
    "    bracket_max = 100.0\n",
    "    # Expand bracket if needed:\n",
    "    while H(bracket_max) < 0:\n",
    "        bracket_max *= 2.0\n",
    "        if bracket_max > 1e12:  # safeguard\n",
    "            break\n",
    "    root_result = root_scalar(H, bracket=[bracket_min, bracket_max], method='bisect')\n",
    "    return root_result.root\n",
    "\n",
    "def estimate_parameters_closed_form(data):\n",
    "    \"\"\"\n",
    "    Estimate (x1, x2) via the partial closed-form approach:\n",
    "      1) Solve for x2 from digamma(x2) = average(log(y2)).\n",
    "      2) Solve for x1 from sum_j y2_j psi(x1 y2_j) = sum_j y2_j log(y1_j).\n",
    "    \"\"\"\n",
    "    x2_hat = solve_for_x2(data)\n",
    "    x1_hat = solve_for_x1(data)\n",
    "    return (x1_hat, x2_hat)\n",
    "\n",
    "\n",
    "def main():\n",
    "    # ------------------------------\n",
    "    # 1. Generate data\n",
    "    # ------------------------------\n",
    "    m = 2000                      # number of samples\n",
    "    true_x = (2.0, 5.0)           # true parameter vector\n",
    "    rng = np.random.default_rng(123)\n",
    "    data = gamma2d_sample(true_x, m, rng=rng)\n",
    "\n",
    "    # ------------------------------\n",
    "    # 2. Baseline log-likelihood at x=[1,1]\n",
    "    # ------------------------------\n",
    "    baseline_x = (1.0, 1.0)\n",
    "    ll_baseline = log_likelihood(baseline_x, data)\n",
    "    print(\"Baseline guess x = (1,1)\")\n",
    "    print(\"Log-likelihood(1,1) =\", ll_baseline)\n",
    "\n",
    "    # ------------------------------\n",
    "    # 3. Estimate x via partial closed-form approach\n",
    "    # ------------------------------\n",
    "    x_hat = estimate_parameters_closed_form(data)\n",
    "    ll_hat = log_likelihood(x_hat, data)\n",
    "    print(\"\\nPartial Closed-Form MLE Approach:\")\n",
    "    print(\"Estimated x =\", x_hat)\n",
    "    print(\"Log-likelihood(x_hat) =\", ll_hat)\n",
    "\n",
    "    # ------------------------------\n",
    "    # 4. Compare to the truth\n",
    "    # ------------------------------\n",
    "    print(\"\\nTrue x         =\", true_x)\n",
    "    print(\"Estimated x    =\", x_hat)\n",
    "    print(\"LL difference  =\", ll_hat - ll_baseline)\n",
    "    print(\"Improvement?   =\", \"Yes\" if ll_hat > ll_baseline else \"No\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## o3-mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = openai_client.chat.completions.create(\n",
    "    model=\"o3-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are an expert in probability theory and stochastic modeling.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    ")\n",
    "\n",
    "# get the response\n",
    "response = completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is one complete solution write‐up that explains our reasoning and then shows a self‐contained Python simulation and optimization procedure.\n",
      "\n",
      "──────────────────────────────\n",
      "1. Explanation and Methodology\n",
      "──────────────────────────────\n",
      "We are given independent observations Yⱼ = (y₁, y₂) generated from the density\n",
      "\n",
      "  f(y₁,y₂; x) = { exp(–y₁)·y₁^(x₁·y₂ – 1) / Γ(x₁·y₂) } · { exp(–y₂)·y₂^(x₂ – 1) / Γ(x₂) },\n",
      "     for y₁, y₂ > 0,\n",
      "\n",
      "with the “true” parameter vector x* = (x₁*, x₂*) unknown (default x* = [2, 5]). In words, y₂ is gamma distributed with shape parameter x₂ and scale 1, and then (conditional on y₂) y₁ is gamma distributed with shape parameter x₁·y₂ and scale 1.\n",
      "\n",
      "A maximum likelihood (ML) approach will maximize the expected log likelihood\n",
      "\n",
      "  g(x) = E[ log f(Y; x) ],\n",
      "     = ∫ log f(y; x) f(y; x*) dy.\n",
      " \n",
      "When we write the log likelihood for a single observation (dropping constants and writing in “nice” form) we have\n",
      "\n",
      "  ℓ(x; y₁,y₂) =\n",
      "    – y₁ – y₂ + (x₁·y₂ – 1)·ln(y₁)\n",
      "       – ln Γ(x₁·y₂)\n",
      "       + (x₂ – 1)·ln(y₂) – ln Γ(x₂).\n",
      "\n",
      "The usual ML procedure would set the derivative(s) with respect to x equal to zero. However, note that the derivative with respect to x₁ involves terms like\n",
      "\n",
      "  d/dx₁ [ (x₁·y₂ – 1) ln(y₁) – ln Γ(x₁·y₂) ]\n",
      "       = y₂·ln(y₁) – y₂ ψ(x₁·y₂),\n",
      "                   \n",
      "where ψ(·) is the digamma function. A similar story holds for x₂. Therefore, even writing the first‐order conditions yields equations that involve ψ and its dependence on the random y₂. In general there is no closed‐form solution for these equations. Thus we must turn to simulation–based methods.\n",
      "\n",
      "──────────────────────────────\n",
      "2. Simulation-Based Approach\n",
      "──────────────────────────────\n",
      "Given that a closed‐form solution is not available, we use a simulation-based optimization method:\n",
      "\n",
      "A. Data Generation\n",
      "  \n",
      " • Generate m i.i.d. observations using the “true” parameters. In our case, using:\n",
      "  – y₂ ~ Gamma(shape = x₂*, scale = 1)\n",
      "  – y₁ | y₂ ~ Gamma(shape = x₁*·y₂, scale = 1).\n",
      "\n",
      "B. The Log Likelihood Function\n",
      "  \n",
      " For a candidate decision variable x = (x₁, x₂) the log likelihood for a single observation is:\n",
      " \n",
      "  ℓ(x; y) = − y₁ − y₂ + (x₁ · y₂ − 1) ln(y₁) − gammaln(x₁ · y₂)\n",
      "       + (x₂ − 1) ln(y₂) − gammaln(x₂)\n",
      " \n",
      " Here, gammaln(·) is used to compute ln Γ(·). We then compute the sample average\n",
      " \n",
      "  G_m(x) = (1/m) Σⱼ ℓ(x; Yⱼ).\n",
      "\n",
      "C. Optimization Strategy\n",
      "  \n",
      "Given a limited evaluation budget (default 1000 function evaluations), we propose a simple direct search:\n",
      " • Randomly sample candidate x values uniformly from (0, 10) × (0, 10). (Recall that the true parameters are known to be within this square.)\n",
      " • For each candidate evaluate G_m(x) using the pre-generated data.\n",
      " • Choose the candidate that yields the highest sample average log likelihood.\n",
      " • For comparison we also compute the log likelihood at the initial guess x = [1, 1].\n",
      "\n",
      "We also compute the sample standard deviation of the log likelihood values (over the m observations) at the optimum candidate in order to provide an estimate of uncertainty for our performance metric.\n",
      "\n",
      "──────────────────────────────\n",
      "3. Python Code\n",
      "──────────────────────────────\n",
      "Below is a complete, self-contained Python code block that implements the above ideas. (Make sure you have the standard packages – numpy and scipy – available.)\n",
      "\n",
      "──────────────────────────────\n",
      "```python\n",
      "import numpy as np\n",
      "from scipy.special import gammaln\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "# -- Settings --\n",
      "# True parameters and starting guess:\n",
      "x_true = np.array([2.0, 5.0])   # x* = [x1*, x2*]\n",
      "x_start = np.array([1.0, 1.0])\n",
      "# Budget for candidate evaluation:\n",
      "BUDGET = 1000  \n",
      "# Number of observations in simulation:\n",
      "m = 1000  \n",
      "# Random seed for reproducibility:\n",
      "np.random.seed(123)\n",
      "\n",
      "# -- Step 1: Data Generation --\n",
      "# Generate observations Y = (y1, y2) as per the model:\n",
      "#   y2 ~ Gamma(shape = x2*, scale = 1)\n",
      "#   y1 | y2 ~ Gamma(shape = x1* * y2, scale = 1)\n",
      "y2_samples = np.random.gamma(shape=x_true[1], scale=1.0, size=m)\n",
      "y1_samples = np.empty(m)\n",
      "for j in range(m):\n",
      "    shape_param = x_true[0] * y2_samples[j]\n",
      "    # In case shape_param is extremely small, numpy gamma can handle shape > 0.\n",
      "    y1_samples[j] = np.random.gamma(shape=shape_param, scale=1.0)\n",
      "\n",
      "# Combine into one dataset (each row is an observation):\n",
      "Y = np.column_stack((y1_samples, y2_samples))\n",
      "\n",
      "# -- Step 2: Define the log likelihood function for a candidate parameter x --\n",
      "def log_likelihood_single(x, y):\n",
      "    \"\"\"\n",
      "    Computes the log likelihood for a single observation y = (y1, y2)\n",
      "    given candidate parameter vector x = [x1, x2].\n",
      "    \"\"\"\n",
      "    y1, y2 = y\n",
      "    x1, x2 = x\n",
      "    term1 = -y1 - y2\n",
      "    term2 = (x1 * y2 - 1.0) * np.log(y1)\n",
      "    term3 = -gammaln(x1 * y2)\n",
      "    term4 = (x2 - 1.0) * np.log(y2)\n",
      "    term5 = -gammaln(x2)\n",
      "    return term1 + term2 + term3 + term4 + term5\n",
      "\n",
      "def sample_log_likelihood(x, Y):\n",
      "    \"\"\"\n",
      "    Returns the average log likelihood over all m observations given candidate x.\n",
      "    Also returns the standard error (std divided by sqrt(m)) of the sample log likelihood.\n",
      "    \"\"\"\n",
      "    logls = np.array([log_likelihood_single(x, y) for y in Y])\n",
      "    avg = np.mean(logls)\n",
      "    std_error = np.std(logls) / np.sqrt(len(Y))\n",
      "    return avg, std_error\n",
      "\n",
      "# -- Step 3: Evaluate the baseline at x_start --\n",
      "baseline_avg, baseline_se = sample_log_likelihood(x_start, Y)\n",
      "print(\"Baseline x =\", x_start, \"Average log likelihood = {:.3f} (SE = {:.3f})\".format(baseline_avg, baseline_se))\n",
      "\n",
      "# -- Step 4: Optimization via Direct Search --\n",
      "# We randomly sample BUDGET candidate solutions from Uniform(0,10) x Uniform(0,10).\n",
      "candidate_x = np.random.uniform(low=0.1, high=10.0, size=(BUDGET, 2))\n",
      "# For storing evaluations:\n",
      "scores = np.zeros(BUDGET)\n",
      "ses = np.zeros(BUDGET)\n",
      "\n",
      "for i in range(BUDGET):\n",
      "    score, se = sample_log_likelihood(candidate_x[i], Y)\n",
      "    scores[i] = score\n",
      "    ses[i] = se\n",
      "\n",
      "# Identify the candidate with maximum average log likelihood:\n",
      "best_index = np.argmax(scores)\n",
      "best_x = candidate_x[best_index]\n",
      "best_score = scores[best_index]\n",
      "best_se = ses[best_index]\n",
      "\n",
      "print(\"Best candidate x = [{:.3f}, {:.3f}] with Average log likelihood = {:.3f} (SE = {:.3f})\".format(\n",
      "    best_x[0], best_x[1], best_score, best_se))\n",
      "\n",
      "# -- Optional: Plot histogram of scores (for visualization) --\n",
      "plt.hist(scores, bins=30, edgecolor='k')\n",
      "plt.xlabel(\"Average Log Likelihood\")\n",
      "plt.ylabel(\"Frequency\")\n",
      "plt.title(\"Histogram of evaluated average log likelihoods\")\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "──────────────────────────────\n",
      "4. Discussion of the Results\n",
      "──────────────────────────────\n",
      "• The code first simulates m observations from the specified two‐dimensional gamma distribution with the “true” parameters x* = [2, 5].  \n",
      "• Next, it defines a function to compute the log likelihood for any candidate parameter x using each observation’s contribution.  \n",
      "• Then, using a simple direct search, the code evaluates 1000 randomly chosen candidate solutions over the region (0, 10) × (0, 10) and picks the candidate that yields the highest sample average log likelihood.  \n",
      "• We also compute the standard error (based on the sample variability) of the estimated log likelihood to gauge uncertainty.  \n",
      "• Finally, the code prints the baseline (the starting guess x = [1, 1]) and shows the improvement by reporting the candidate with the best log likelihood.\n",
      "\n",
      "Typically with a reasonable simulation size (m = 1000) and budget (BUDGET = 1000 evaluations), you should observe that the best candidate “x” is close to the true parameters [2, 5]. (Because of sample variability and randomness in the candidate draw, your estimate may be, for example, approximately x ≈ [1.95, 5.1].)\n",
      "\n",
      "──────────────────────────────\n",
      "5. Conclusion and Recommended Estimate\n",
      "──────────────────────────────\n",
      "Based on the simulation‐based ML estimation described above, our recommended procedure is to:\n",
      " 1. Generate a dataset using the true model.\n",
      " 2. Use direct search over the candidate space (0, 10)×(0, 10) (with a budget of 1000 evaluations) to estimate Gₘ(x).\n",
      " 3. Pick the candidate solution that maximizes Gₘ(x).\n",
      "\n",
      "In our example, the best candidate parameter (x̂) we found had an average log likelihood higher than that of the initial guess x = [1, 1]. Numerical results might look like:\n",
      "\n",
      " Baseline x = [1, 1], Avg. LL = –[A] (SE = [B])  \n",
      " Best candidate x = [1.95, 5.12], Avg. LL = –[C] (SE = [D])\n",
      "\n",
      "(Where the numbers in [ ] are sample‐dependent.) The estimate x̂ is acceptable as it lies within the prescribed square (0, 10)×(0, 10) and demonstrates clear improvement over the baseline.\n",
      "\n",
      "This simulation approach both verifies that a closed‐form solution is not available and provides a practical method to recover x* from simulated data under the given constraints.\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline x = [1. 1.] Average log likelihood = -9.218 (SE = 0.113)\n",
      "Best candidate x = [2.111, 5.526] with Average log likelihood = -4.646 (SE = 0.030)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.special import gammaln\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# -- Settings --\n",
    "# True parameters and starting guess:\n",
    "x_true = np.array([2.0, 5.0])   # x* = [x1*, x2*]\n",
    "x_start = np.array([1.0, 1.0])\n",
    "# Budget for candidate evaluation:\n",
    "BUDGET = 1000  \n",
    "# Number of observations in simulation:\n",
    "m = 1000  \n",
    "# Random seed for reproducibility:\n",
    "np.random.seed(123)\n",
    "\n",
    "# -- Step 1: Data Generation --\n",
    "# Generate observations Y = (y1, y2) as per the model:\n",
    "#   y2 ~ Gamma(shape = x2*, scale = 1)\n",
    "#   y1 | y2 ~ Gamma(shape = x1* * y2, scale = 1)\n",
    "y2_samples = np.random.gamma(shape=x_true[1], scale=1.0, size=m)\n",
    "y1_samples = np.empty(m)\n",
    "for j in range(m):\n",
    "    shape_param = x_true[0] * y2_samples[j]\n",
    "    # In case shape_param is extremely small, numpy gamma can handle shape > 0.\n",
    "    y1_samples[j] = np.random.gamma(shape=shape_param, scale=1.0)\n",
    "\n",
    "# Combine into one dataset (each row is an observation):\n",
    "Y = np.column_stack((y1_samples, y2_samples))\n",
    "\n",
    "# -- Step 2: Define the log likelihood function for a candidate parameter x --\n",
    "def log_likelihood_single(x, y):\n",
    "    \"\"\"\n",
    "    Computes the log likelihood for a single observation y = (y1, y2)\n",
    "    given candidate parameter vector x = [x1, x2].\n",
    "    \"\"\"\n",
    "    y1, y2 = y\n",
    "    x1, x2 = x\n",
    "    term1 = -y1 - y2\n",
    "    term2 = (x1 * y2 - 1.0) * np.log(y1)\n",
    "    term3 = -gammaln(x1 * y2)\n",
    "    term4 = (x2 - 1.0) * np.log(y2)\n",
    "    term5 = -gammaln(x2)\n",
    "    return term1 + term2 + term3 + term4 + term5\n",
    "\n",
    "def sample_log_likelihood(x, Y):\n",
    "    \"\"\"\n",
    "    Returns the average log likelihood over all m observations given candidate x.\n",
    "    Also returns the standard error (std divided by sqrt(m)) of the sample log likelihood.\n",
    "    \"\"\"\n",
    "    logls = np.array([log_likelihood_single(x, y) for y in Y])\n",
    "    avg = np.mean(logls)\n",
    "    std_error = np.std(logls) / np.sqrt(len(Y))\n",
    "    return avg, std_error\n",
    "\n",
    "# -- Step 3: Evaluate the baseline at x_start --\n",
    "baseline_avg, baseline_se = sample_log_likelihood(x_start, Y)\n",
    "print(\"Baseline x =\", x_start, \"Average log likelihood = {:.3f} (SE = {:.3f})\".format(baseline_avg, baseline_se))\n",
    "\n",
    "# -- Step 4: Optimization via Direct Search --\n",
    "# We randomly sample BUDGET candidate solutions from Uniform(0,10) x Uniform(0,10).\n",
    "candidate_x = np.random.uniform(low=0.1, high=10.0, size=(BUDGET, 2))\n",
    "# For storing evaluations:\n",
    "scores = np.zeros(BUDGET)\n",
    "ses = np.zeros(BUDGET)\n",
    "\n",
    "for i in range(BUDGET):\n",
    "    score, se = sample_log_likelihood(candidate_x[i], Y)\n",
    "    scores[i] = score\n",
    "    ses[i] = se\n",
    "\n",
    "# Identify the candidate with maximum average log likelihood:\n",
    "best_index = np.argmax(scores)\n",
    "best_x = candidate_x[best_index]\n",
    "best_score = scores[best_index]\n",
    "best_se = ses[best_index]\n",
    "\n",
    "print(\"Best candidate x = [{:.3f}, {:.3f}] with Average log likelihood = {:.3f} (SE = {:.3f})\".format(\n",
    "    best_x[0], best_x[1], best_score, best_se))\n",
    "\n",
    "# -- Optional: Plot histogram of scores (for visualization) --\n",
    "plt.hist(scores, bins=30, edgecolor='k')\n",
    "plt.xlabel(\"Average Log Likelihood\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Histogram of evaluated average log likelihoods\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Claude 3.5 Sonnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'll help solve this problem using a comprehensive approach. Let me break down the solution:\n",
      "\n",
      "1. Analysis of the Problem Structure:\n",
      "The density function represents a hierarchical gamma model where:\n",
      "- y₂ follows a Gamma(x*₂, 1) distribution\n",
      "- Given y₂, y₁ follows a Gamma(x*₁y₂, 1) distribution\n",
      "\n",
      "2. Approach Selection:\n",
      "Given the complex nature of the joint distribution and the involvement of gamma functions, a purely analytical solution would be challenging. Therefore, I'll propose a simulation-based maximum likelihood estimation approach.\n",
      "\n",
      "Here's a complete solution implementation:\n",
      "\n",
      "```python\n",
      "import numpy as np\n",
      "from scipy.stats import gamma\n",
      "from scipy.optimize import minimize\n",
      "from scipy.special import gammaln\n",
      "\n",
      "def generate_samples(x_true, m):\n",
      "    \"\"\"Generate m samples from the two-dimensional gamma distribution\"\"\"\n",
      "    y2 = gamma.rvs(a=x_true[1], scale=1.0, size=m)\n",
      "    y1 = np.array([gamma.rvs(a=x_true[0]*y2[i], scale=1.0) for i in range(m)])\n",
      "    return np.column_stack((y1, y2))\n",
      "\n",
      "def log_likelihood(x, samples):\n",
      "    \"\"\"Compute the negative log likelihood for parameter vector x\"\"\"\n",
      "    y1, y2 = samples[:, 0], samples[:, 1]\n",
      "    \n",
      "    # Log likelihood for y1 given y2\n",
      "    ll_y1 = np.sum((x[0]*y2 - 1)*np.log(y1) - y1 - gammaln(x[0]*y2))\n",
      "    \n",
      "    # Log likelihood for y2\n",
      "    ll_y2 = np.sum((x[1] - 1)*np.log(y2) - y2 - gammaln(x[1]))\n",
      "    \n",
      "    return -(ll_y1 + ll_y2)\n",
      "\n",
      "def estimate_parameters(samples, x0=[1.0, 1.0]):\n",
      "    \"\"\"Estimate parameters using maximum likelihood\"\"\"\n",
      "    bounds = [(0.1, 10), (0.1, 10)]  # Constrain parameters to (0,10)\n",
      "    \n",
      "    result = minimize(\n",
      "        lambda x: log_likelihood(x, samples),\n",
      "        x0,\n",
      "        bounds=bounds,\n",
      "        method='L-BFGS-B'\n",
      "    )\n",
      "    \n",
      "    return result.x\n",
      "\n",
      "# Simulation parameters\n",
      "np.random.seed(42)\n",
      "m = 1000  # number of samples\n",
      "x_true = np.array([2.0, 5.0])  # true parameters\n",
      "x0 = np.array([1.0, 1.0])  # initial guess\n",
      "\n",
      "# Generate samples\n",
      "samples = generate_samples(x_true, m)\n",
      "\n",
      "# Estimate parameters\n",
      "x_est = estimate_parameters(samples, x0)\n",
      "\n",
      "# Compute log-likelihoods\n",
      "ll_initial = -log_likelihood(x0, samples)\n",
      "ll_estimated = -log_likelihood(x_est, samples)\n",
      "ll_true = -log_likelihood(x_true, samples)\n",
      "\n",
      "print(f\"True parameters: {x_true}\")\n",
      "print(f\"Estimated parameters: {x_est}\")\n",
      "print(f\"Initial log-likelihood: {ll_initial:.2f}\")\n",
      "print(f\"Estimated log-likelihood: {ll_estimated:.2f}\")\n",
      "print(f\"True log-likelihood: {ll_true:.2f}\")\n",
      "\n",
      "# Bootstrap for confidence intervals\n",
      "n_bootstrap = 100\n",
      "bootstrap_estimates = np.zeros((n_bootstrap, 2))\n",
      "\n",
      "for i in range(n_bootstrap):\n",
      "    indices = np.random.choice(m, size=m, replace=True)\n",
      "    bootstrap_samples = samples[indices]\n",
      "    bootstrap_estimates[i] = estimate_parameters(bootstrap_samples, x_est)\n",
      "\n",
      "confidence_intervals = np.percentile(bootstrap_estimates, [2.5, 97.5], axis=0)\n",
      "print(\"\\nBootstrap 95% Confidence Intervals:\")\n",
      "print(f\"x1: ({confidence_intervals[0,0]:.2f}, {confidence_intervals[1,0]:.2f})\")\n",
      "print(f\"x2: ({confidence_intervals[0,1]:.2f}, {confidence_intervals[1,1]:.2f})\")\n",
      "```\n",
      "\n",
      "3. Key Components of the Solution:\n",
      "\n",
      "a) Sample Generation:\n",
      "- Implements the hierarchical structure of the model\n",
      "- Uses scipy.stats.gamma for efficient random number generation\n",
      "\n",
      "b) Log-likelihood Computation:\n",
      "- Implements the full log-likelihood function\n",
      "- Uses gammaln for numerical stability\n",
      "- Combines both components of the joint distribution\n",
      "\n",
      "c) Parameter Estimation:\n",
      "- Uses L-BFGS-B optimization with bounds\n",
      "- Includes constraints to ensure parameters stay in (0,10)\n",
      "- Starts from the initial guess [1,1]\n",
      "\n",
      "d) Uncertainty Quantification:\n",
      "- Implements bootstrap resampling\n",
      "- Provides confidence intervals for parameter estimates\n",
      "\n",
      "4. Validation and Performance:\n",
      "\n",
      "The method:\n",
      "- Consistently recovers parameters close to the true values\n",
      "- Shows significant improvement over the initial guess\n",
      "- Provides uncertainty estimates through bootstrap\n",
      "- Stays within the feasible region (0,10)×(0,10)\n",
      "- Requires minimal tuning\n",
      "- Is computationally efficient\n",
      "\n",
      "5. Recommendations:\n",
      "\n",
      "- Use multiple random starts if computational budget allows\n",
      "- Increase sample size m if more precision is needed\n",
      "- Adjust bounds if prior knowledge suggests different parameter ranges\n",
      "- Monitor convergence of the optimizer\n",
      "\n",
      "This solution meets all the success criteria and provides a robust framework for parameter estimation in this two-dimensional gamma distribution model.\n"
     ]
    }
   ],
   "source": [
    "response = claude_client.messages.create(\n",
    "    model=\"claude-3-5-sonnet-20241022\",  # Or another Claude 3 model\n",
    "    max_tokens=4096,\n",
    "    system=\"You are an expert in probability theory and stochastic modeling.\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.content[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True parameters: [2. 5.]\n",
      "Estimated parameters: [2.00432335 5.12576312]\n",
      "Initial log-likelihood: -9293.76\n",
      "Estimated log-likelihood: -4654.80\n",
      "True log-likelihood: -4656.55\n",
      "\n",
      "Bootstrap 95% Confidence Intervals:\n",
      "x1: (1.98, 2.04)\n",
      "x2: (5.01, 5.25)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import gamma\n",
    "from scipy.optimize import minimize\n",
    "from scipy.special import gammaln\n",
    "\n",
    "def generate_samples(x_true, m):\n",
    "    \"\"\"Generate m samples from the two-dimensional gamma distribution\"\"\"\n",
    "    y2 = gamma.rvs(a=x_true[1], scale=1.0, size=m)\n",
    "    y1 = np.array([gamma.rvs(a=x_true[0]*y2[i], scale=1.0) for i in range(m)])\n",
    "    return np.column_stack((y1, y2))\n",
    "\n",
    "def log_likelihood(x, samples):\n",
    "    \"\"\"Compute the negative log likelihood for parameter vector x\"\"\"\n",
    "    y1, y2 = samples[:, 0], samples[:, 1]\n",
    "    \n",
    "    # Log likelihood for y1 given y2\n",
    "    ll_y1 = np.sum((x[0]*y2 - 1)*np.log(y1) - y1 - gammaln(x[0]*y2))\n",
    "    \n",
    "    # Log likelihood for y2\n",
    "    ll_y2 = np.sum((x[1] - 1)*np.log(y2) - y2 - gammaln(x[1]))\n",
    "    \n",
    "    return -(ll_y1 + ll_y2)\n",
    "\n",
    "def estimate_parameters(samples, x0=[1.0, 1.0]):\n",
    "    \"\"\"Estimate parameters using maximum likelihood\"\"\"\n",
    "    bounds = [(0.1, 10), (0.1, 10)]  # Constrain parameters to (0,10)\n",
    "    \n",
    "    result = minimize(\n",
    "        lambda x: log_likelihood(x, samples),\n",
    "        x0,\n",
    "        bounds=bounds,\n",
    "        method='L-BFGS-B'\n",
    "    )\n",
    "    \n",
    "    return result.x\n",
    "\n",
    "# Simulation parameters\n",
    "np.random.seed(42)\n",
    "m = 1000  # number of samples\n",
    "x_true = np.array([2.0, 5.0])  # true parameters\n",
    "x0 = np.array([1.0, 1.0])  # initial guess\n",
    "\n",
    "# Generate samples\n",
    "samples = generate_samples(x_true, m)\n",
    "\n",
    "# Estimate parameters\n",
    "x_est = estimate_parameters(samples, x0)\n",
    "\n",
    "# Compute log-likelihoods\n",
    "ll_initial = -log_likelihood(x0, samples)\n",
    "ll_estimated = -log_likelihood(x_est, samples)\n",
    "ll_true = -log_likelihood(x_true, samples)\n",
    "\n",
    "print(f\"True parameters: {x_true}\")\n",
    "print(f\"Estimated parameters: {x_est}\")\n",
    "print(f\"Initial log-likelihood: {ll_initial:.2f}\")\n",
    "print(f\"Estimated log-likelihood: {ll_estimated:.2f}\")\n",
    "print(f\"True log-likelihood: {ll_true:.2f}\")\n",
    "\n",
    "# Bootstrap for confidence intervals\n",
    "n_bootstrap = 100\n",
    "bootstrap_estimates = np.zeros((n_bootstrap, 2))\n",
    "\n",
    "for i in range(n_bootstrap):\n",
    "    indices = np.random.choice(m, size=m, replace=True)\n",
    "    bootstrap_samples = samples[indices]\n",
    "    bootstrap_estimates[i] = estimate_parameters(bootstrap_samples, x_est)\n",
    "\n",
    "confidence_intervals = np.percentile(bootstrap_estimates, [2.5, 97.5], axis=0)\n",
    "print(\"\\nBootstrap 95% Confidence Intervals:\")\n",
    "print(f\"x1: ({confidence_intervals[0,0]:.2f}, {confidence_intervals[1,0]:.2f})\")\n",
    "print(f\"x2: ({confidence_intervals[0,1]:.2f}, {confidence_intervals[1,1]:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DeepSeek-R1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated parameters: [2.00432319 5.12575288]\n",
      "True parameters: [2, 5]\n",
      "Optimization success: True\n",
      "Message: CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.special import gammaln, digamma\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# Generate synthetic data\n",
    "def generate_data(m, x_true):\n",
    "    np.random.seed(42)\n",
    "    y2 = np.random.gamma(shape=x_true[1], scale=1, size=m)\n",
    "    y1 = np.array([np.random.gamma(shape=x_true[0] * y2j, scale=1) for y2j in y2])\n",
    "    return np.column_stack((y1, y2))\n",
    "\n",
    "# Negative log-likelihood function\n",
    "def neg_log_likelihood(x, data):\n",
    "    x1, x2 = x\n",
    "    y1, y2 = data[:, 0], data[:, 1]\n",
    "    shape_y1 = x1 * y2\n",
    "    term1 = -y1 + (shape_y1 - 1) * np.log(y1) - gammaln(shape_y1)\n",
    "    term2 = -y2 + (x2 - 1) * np.log(y2) - gammaln(x2)\n",
    "    return -np.mean(term1 + term2)\n",
    "\n",
    "# Gradient of the negative log-likelihood\n",
    "def gradient(x, data):\n",
    "    x1, x2 = x\n",
    "    y1, y2 = data[:, 0], data[:, 1]\n",
    "    m = len(data)\n",
    "    \n",
    "    # Gradient for x1\n",
    "    d_x1 = np.mean(y2 * (np.log(y1) - digamma(x1 * y2)))\n",
    "    \n",
    "    # Gradient for x2\n",
    "    d_x2 = np.mean(np.log(y2) - digamma(x2))\n",
    "    \n",
    "    return np.array([-d_x1, -d_x2])\n",
    "\n",
    "# True parameters and data generation\n",
    "x_true = [2, 5]\n",
    "m = 1000  # Sample size\n",
    "data = generate_data(m, x_true)\n",
    "\n",
    "# Initial guess and optimization\n",
    "result = minimize(neg_log_likelihood, x0=[1, 1], args=(data,),\n",
    "                  method='L-BFGS-B', jac=gradient,\n",
    "                  bounds=[(1e-6, None), (1e-6, None)])\n",
    "\n",
    "# Results\n",
    "print(f\"Estimated parameters: {result.x}\")\n",
    "print(f\"True parameters: {x_true}\")\n",
    "print(f\"Optimization success: {result.success}\")\n",
    "print(f\"Message: {result.message}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-simopt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
